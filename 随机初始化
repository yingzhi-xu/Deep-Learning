随机初始化权重：对于logistic回归，可以将权重初始化为0；但如果将神将网络的各参数全部初始化为0，再使用梯度下降算法，就会无效。

原因：
e.g. 两个输入特征，两个隐藏单元

w^[1](2×2)、b^[1](2×1)初始化为0

这种初始化的问题在于：给网络输入任何样本，a^[1]_1和a^[1]_2是一样的 ——> 两个激活函数一样
当进行反向传播时，由于对称性dz^[1]_1和dz^[1]_2也是一样的

完全对称：意味着节点计算完全一样的函数

两个隐藏单元从一开始就做相同的计算，两个隐藏单元对输出单元的影响也一样大，在迭代一次之后对称性依然存在。
无论训练多长时间，两个隐藏单元计算完全一样的函数，在这种情况下，多个隐藏单元毫无意义。

解决方法：随机初始化
令W^[1]=np.random.randn((2,2))*0.01
  b^[1]=np.zeros((2,1))
  
为什么乘0.01？
习惯将w初始化为非常小的随机值。
Z^[1]=W^[1]X+b^[1]
a^[1]=g^[1](Z^[1])
如果W很大，z就很大。在tanh或sigmoid函数中就会落到平缓的部分。即如果w太大，在一开始训练时就落在z很大的区域，导致tanh或sigmoid函数接近饱和，从而减慢学习速度。

（当你训练一个很深的神经网络时，要试试0.01以外的数。）
